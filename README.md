# Small Language Model (SLM)

This project is an experimental implementation of a **Small Language Model (SLM)** built from scratch. The goal is to learn and demonstrate the core concepts behind training and running large language modelsâ€”on a smaller scale that is easier to understand, train, and run on consumer hardware.  

## Features
- Tokenization and vocabulary building  
- Embedding layers for token representation  
- Transformer-style architecture (multi-head attention + feedforward)  
- Training loop with loss tracking  
- Sampling and text generation  
- Configurable hyperparameters (layers, hidden size, vocab size, etc.)  

## Learning Goals
- Understand how tokenization and embeddings work  
- Implement attention and transformer blocks from scratch  
- Train a small but functional autoregressive model  
- Explore scaling laws and limitations of small models  

## Project Goals 
- Potential About me chat feature on Portfolio Website
  

