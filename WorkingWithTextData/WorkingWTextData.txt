


Everything that has been implemented in this folder was the steps to preparing raw text for 
the GPT based transformer model to be ready training. 

Summary:
    ---    LLMs require textual data to be converted into numerical vectors, known as
        embeddings, since they can’t process raw text. Embeddings transform discrete
        data (like words or images) into continuous vector spaces, making them compatible
        with neural network operations.

    ---    As the first step, raw text is broken into tokens, which can be words or characters.
        Then, the tokens are converted into integer representations, termed token IDs.

    ---    Special tokens, such as <|unk|> and <|endoftext|>, can be added to enhance
        the model’s understanding and handle various contexts, such as unknown
        words or marking the boundary between unrelated texts.

    ---    The byte pair encoding (BPE) tokenizer used for LLMs like GPT-2 and GPT-3
        can efficiently handle unknown words by breaking them down into subword
        units or individual characters.

    ---    We use a sliding window approach on tokenized data to generate input–target
        pairs for LLM training.

    ---    Embedding layers in PyTorch function as a lookup operation, retrieving vectors
        corresponding to token IDs. The resulting embedding vectors provide continuous
        representations of tokens, which is crucial for training deep learning models
        like LLMs.

    ---    While token embeddings provide consistent vector representations for each
        token, they lack a sense of the token’s position in a sequence. To rectify this,
        two main types of positional embeddings exist: absolute and relative. OpenAI’s
        GPT models utilize absolute positional embeddings, which are added to the token
        embedding vectors and are optimized during the model training.  



Next ---> Coding Attention Mechanisms