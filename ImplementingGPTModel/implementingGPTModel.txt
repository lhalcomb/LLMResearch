The contents of this folder include the necessary knowledge in a GPT-like 
llm model from scratch to generate text. 

The bulk of this chapter is very dense. Pay close attention, haha attention. 
I feel like I should cite the paper everytime I say the word. -- Layden H.

We will cover the following: 

    -- Coding a GPT-like large language model (LLM)
        that can be trained to generate human-like text

    -- Normalizing layer activations to stabilize neural
    network training

    -- Adding shortcut connections in deep neural
    networks

    -- Implementing transformer blocks to create GPT
    models of various sizes
    
    -- Computing the number of parameters and
    storage requirements of GPT models


The following folder contains much information that can be found in research papers across the internet
Here are the references. 

References: 

   "Language Models Are Unsupervised Multitask Learners,‚Äù by Radford et al. (https://mng.bz/yoBq).

Summary: 