""" 

This file contains the contents to the beginning of pretraining on unlabled data
for the LLM. We are using a context window of 256. This is because most modern machines 
cannot handle the 1024 context window size, for it causes the transformer to deal 
with a lot of heinous and tedious calculations that causes memory overflow on modern 
DDRAM. (when running on cpu) 
If we recall from ch. 4, calculating these parameters is done by the following, 

vocab_size x emd_dim + n_layers x n_heads x emb_dim^2 = 124,412,160 -> 1024 token size
vocab_size x emd_dim + n_layers x n_heads x emb_dim^2 = 123,822,336 -> 256 token size

not much, but when training it can get some crazy overhead in our DDRAM. 

 Originally, the GPT-2 model with 124 million parameters was configured to handle
up to 1,024 tokens. After the training process, we will update the context size setting
and load pretrained weights to work with a model configured for a 1,024-token context length
"""
# Add the top-level project directory (/Users/laydenhalcomb/LLMResearch)
import sys
import os
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "ImplementingGPTModel"))
sys.path.append(project_root)

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import tiktoken
from GPTModel import GPTModel
tokenizer = tiktoken.get_encoding("gpt2")


GPT_CONFIG_124M = {
 "vocab_size": 50257,
 "context_length": 256,
 "emb_dim": 768,
 "n_heads": 12,
 "n_layers": 12,
 "drop_rate": 0.1,
 "qkv_bias": False
}
torch.manual_seed(123)
model = GPTModel(GPT_CONFIG_124M)
model.eval()

#5.1.1 Using GPT to generate text

#development of the text_to_token_ids and token_ids_to_text from utility file
from utility import text_to_token_ids, token_ids_to_text, generate_text_simple

# start_context = "Every effort moves you"
# token_ids = generate_text_simple(
#  model=model,
#  idx=text_to_token_ids(start_context, tokenizer),
#  max_new_tokens=10,
#  context_size=GPT_CONFIG_124M["context_length"]
# )

#print("Output text:\n", token_ids_to_text(token_ids, tokenizer))

"""
Next, we will calculate a loss metric for the generated outputs. This loss serves as a
progress and success indicator of the training progress. Furthermore, in later chapters, 
when we fine-tune our LLM, we will review additional methodologies for assessing model quality.



Now we will look at techniques for numerically assessing text quality generated 
during training, starting with a recap of data loading and text generation.

The flow of input text to llm generated text is done in 5 steps:

1. Use vocabulary to map the input text to token IDs

2. Obtain probability row vector for each input token via softmax
the function.

3. Locate the index position with the highest probability value in each row vector, which
is done via the argmax function. 

4.  Obtain all predicted token IDs as the index positions with the
highest probabilities.

5. Map index positions back into text via the inverse
vocabulary.

The output text generated by the LLM

"""

#5.1.2 Calculating the text generation loss

#step 1
#simple examples from inputs to target text
inputs = torch.tensor([[16833, 3626, 6100], # ["every effort moves",
                        [40, 1107, 588]]) # "I really like"]

targets = torch.tensor([[3626, 6100, 345 ], # [" effort moves you",
                        [1107, 588, 11311]]) # " really like chocolate"]

#notice the shifting, this strategy is crucial for teaching the model to predict the next token in a sequence

#here we feed the inputs into the model, calculate logit vectors, then apply softmax 

#step 2
with torch.no_grad():
    logits = model(inputs) 
probas = torch.softmax(logits, dim=-1)

#print(probas.shape) #torch.Size([num of rows in inputs (batchsize), num of tokens in each batch size, emb_dim = vocab_size])
#step 3 - the argmax
token_ids = torch.argmax(probas, dim=-1, keepdim=True)
#step 4 - obtaining tokenids
#print("Token IDs:\n", token_ids)

#which yields s two sets of outputs, each with three predicted token IDs

"""
Token IDs:
 tensor([[[16657],
         [  339],
         [42826]],

        [[49906],
         [29669],
         [41751]]])

"""

#step 5  - convert tokens into text
#print(f"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}")
#print(f"Outputs batch 1:" f" {token_ids_to_text(token_ids[0].flatten(), tokenizer)}")


"""
Targets batch 1:  effort moves you
Outputs batch 1:  Armed heNetflix

bc of lack of training, we get goobley gook. But, thats next
"""

#text evalutation

#for the next parts i will no longer put in depth comments, to follow allong correctly 
#read from the text from p. 135 - 168


"""
The model training aims to increase the softmax probability in the index positions
corresponding to the correct target token IDs, as illustrated in figure 5.6. This softmax
probability is also used in the evaluation metric we will implement next to numerically
assess the model's generated outputs: the higher the probability in the correct positions, the better.

"""

text_idx = 0
target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]
#print("Text 1:", target_probas_1)

text_idx = 1
target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]
#print("Text 2:", target_probas_2)

"""
this corresponds to the 50,257 vocab size 
so a 50% probability for a given word would be (.5 x 50257 / 50257)
Text 1: tensor([7.4541e-05, 3.1061e-05, 1.1563e-05])
Text 2: tensor([1.0337e-05, 5.6776e-05, 4.7559e-06])
"""


"""
In your machine learning course, you learned backpropogation. This is core
in training. We update model weights so that the model outputs higher values for the 
repective token IDs we want to generate. This is done through backpropogation. 

Backpropagation requires a loss function, which calculates the difference between
the model’s predicted output (here, the probabilities corresponding to the target
token IDs) and the actual desired output. This loss function measures how far off the
model’s predictions are from the target values.
"""


#calculating the loss

log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))
#print(log_probas) #tensor([ -9.5042, -10.3796, -11.3677, -11.4798,  -9.7764, -12.2561])

#see figure 5.7 for the process 

avg_log_probas = torch.mean(log_probas)
#print(avg_log_probas) #tensor(-10.7940)

neg_avg_log_probas = avg_log_probas * -1
#print(neg_avg_log_probas) #tensor(10.7940)

"""
Cross entropy loss

    At its core, the cross entropy loss is a popular measure in machine learning and deep
learning that measures the difference between two probability distributions—typically,
the true distribution of labels (here, tokens in a dataset) and the predicted distribution from a model 
(for instance, the token probabilities generated by an LLM).

    In the context of machine learning and specifically in frameworks like PyTorch, the
cross_entropy function computes this measure for discrete outcomes, which is
similar to the negative average log probability of the target tokens given the model’s
generated token probabilities, making the terms “cross entropy” and “negative average log probability” 
related and often used interchangeably in practice.

"""

#print("Logits shape:", logits.shape) # Logits shape: torch.Size([2, 3, 50257])
#print("Targets shape:", targets.shape) # Targets shape: torch.Size([2, 3])

logits_flat = logits.flatten(0, 1)
targets_flat = targets.flatten()
#print("Flattened logits:", logits_flat.shape) #Flattened logits: torch.Size([6, 50257])
#print("Flattened targets:", targets_flat.shape)#Flattened targets: torch.Size([6])

# this takes care of all the steps we did previously but we needed to do that previously to understand this use
loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)
#print(loss) # tensor(10.7940), same as before

"""
Perplexity is a measure often used alongside cross entropy loss to evaluate the performance of models in tasks like language modeling. 
It can provide a more interpretable way to understand the uncertainty of a model in predicting the next token in a
sequence.

Perplexity measures how well the probability distribution predicted by the model
matches the actual distribution of the words in the dataset. Similar to the loss, a lower
perplexity indicates that the model predictions are closer to the actual distribution.

Perplexity can be calculated as perplexity = torch.exp(loss), which returns
tensor(48725.8203) when applied to the previously calculated loss.
Perplexity is often considered more interpretable than the raw loss value because it signifies the 
effective vocabulary size about which the model is uncertain at each step. In
the given example, this would translate to the model being unsure about which among
48,725 tokens in the vocabulary to generate as the next token.

"""

#5.1.3 Calculating the training and validation set losses
#training & validation losses

#we will work with the verdict.txt to train the model

file_path = "the-verdict.txt"
with open(file_path, "r", encoding="utf-8") as file:
    text_data = file.read()

total_characters = len(text_data)
total_tokens = len(tokenizer.encode(text_data))
print("Characters:", total_characters) #20,479
print("Tokens:", total_tokens) #5,145

"""
With just 5,145 tokens, the text might seem too small to train an LLM, but as mentioned earlier, it’s for educational purposes 
so that we can run the code in minutes instead of weeks.
Plus, later we will load pretrained weights from OpenAI into our GPTModel code.

Next, we divide the dataset into a training and a validation set and use the data
loaders from chapter 2 to prepare the batches for LLM training. This process is visualized in figure 5.9. Due to spatial constraints, we use a max_length=6. However, for the
actual data loaders, we set the max_length equal to the 256-token context length that
the LLM supports so that the LLM sees longer texts during training.

see figure 5.9 pg.142 for illustration

"""

#we are doing a 90 10 split train to test respectively
dataloaderPath = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "ch2_WorkingWithTextData"))
sys.path.append(dataloaderPath)
from GPTDatasetV1 import GPTDatasetV1
train_ratio = 0.90
split_idx = int(train_ratio * len(text_data))
train_data = text_data[:split_idx]
val_data = text_data[split_idx:]

def create_dataloader_v1(txt, batch_size=4, max_length=256,
                        stride=128, shuffle=True, drop_last=True,
                        num_workers=0):
    tokenizer = tiktoken.get_encoding("gpt2")
    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)
    dataloader = DataLoader(
                    dataset,
                    batch_size=batch_size,
                    shuffle=shuffle,
                    drop_last=drop_last,
                    num_workers=num_workers
                )
    return dataloader

torch.manual_seed(123)
train_loader = create_dataloader_v1(
 train_data,
 batch_size=2,
 max_length=GPT_CONFIG_124M["context_length"],
 stride=GPT_CONFIG_124M["context_length"],
 drop_last=True,
 shuffle=True,
 num_workers=0
)
val_loader = create_dataloader_v1(
 val_data,
 batch_size=2,
 max_length=GPT_CONFIG_124M["context_length"],
 stride=GPT_CONFIG_124M["context_length"],
 drop_last=False,
 shuffle=False,
 num_workers=0
)

#typically you want a batchsize around 1024, but im not ceo of meta and i have macbook so we're using batchsize 2

# print("Train loader:")
# for x, y in train_loader:
#  print(x.shape, y.shape)
# #prints 9 (bc 90% of training data) training set batches

# print("\nValidation loader:")
# for x, y in val_loader:
#  print(x.shape, y.shape)
#prints 1  (bc 10% of training data) validation set batches

from utility import train_model_simple

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# model.to(device)

# with torch.no_grad():
#  train_loss = calc_loss_loader(train_loader, model, device)
#  val_loss = calc_loss_loader(val_loader, model, device)
# print("Training loss:", train_loss) #Training loss: 10.98758347829183
# print("Validation loss:", val_loss) #Validation loss: 10.98110580444336

#rn the model isn't training, but now that we have a way to calculate this lets get to training 

#5.2 Training an LLM

#See pg. 146 for the training loop process. It follows the same computational learning theory we are used to

torch.manual_seed(123)
model = GPTModel(GPT_CONFIG_124M)
model.to(device)

"""
Adam optimizers are a popular choice for training deep neural networks. However, in
our training loop, we opt for the AdamW optimizer. 

AdamW is a variant of Adam that improves the weight decay approach, which aims to minimize model complexity and
prevent overfitting by penalizing larger weights. This adjustment allows AdamW to
achieve more effective regularization and better generalization; thus, AdamW is frequently used in the training of LLMs.
"""


optimizer = torch.optim.AdamW(
 model.parameters(),
 lr=0.0004, weight_decay=0.1
)

num_epochs = 10
train_losses, val_losses, tokens_seen = train_model_simple(
 model, train_loader, val_loader, optimizer, device,
 num_epochs=num_epochs, eval_freq=5, eval_iter=5,
 start_context="Every effort moves you", tokenizer=tokenizer
)

#when you run this, you'll notice the val_loss get stuck. We will fix this soon


#Here is a plot of the training and validation set side by side
# import matplotlib.pyplot as plt
# from matplotlib.ticker import MaxNLocator
# def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):
#     fig, ax1 = plt.subplots(figsize=(5, 3))
#     ax1.plot(epochs_seen, train_losses, label="Training loss")
#     ax1.plot(
#     epochs_seen, val_losses, linestyle="-.", label="Validation loss"
#     )
#     ax1.set_xlabel("Epochs")
#     ax1.set_ylabel("Loss")
#     ax1.legend(loc="upper right")
#     ax1.xaxis.set_major_locator(MaxNLocator(integer=True))
#     ax2 = ax1.twiny() #creates a second x-axis that shares the same y-axis
#     ax2.plot(tokens_seen, train_losses, alpha=0) #invisible plot for aligning ticks
#     ax2.set_xlabel("Tokens seen")
#     fig.tight_layout()
#     plt.show()

# epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))
# plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)

"""
As we can see, both the training and validation losses start to improve for the first epoch. However,
the losses start to diverge past the second epoch.

This divergence and the fact that the validation loss is much larger than the training loss indicate that 
the model is overfitting to the training data. We can confirm that the model memorizes the training data
verbatim by searching for the generated text snippets, such as quite insensible to
the irony in the “The Verdict” text file.

This memorization is expected since we are working with a very, very small training
dataset and training the model for multiple epochs. Usually, it’s common to train a
model on a much larger dataset for only one epoch. -(Hold on, ONLY ONE?!)
"""

#see appendix B  for training the model on
#60,000 public domain books from Project Gutenberg, where this overfitting
#does not occur

"""
We have completed four of our objectives for this chapter.
Next, we will cover text generation strategies for LLMs to reduce training data memorization and increase the originality of the LLM-generated text before we cover weight
loading and saving and loading pretrained weights from OpenAI's GPT model.
"""

"""

See RandomnessControl.py


We can now apply the temperature scaling and multinomial function for probabilistic
sampling to select the next token among these three non-zero probability scores to
generate the next token. We do this next by modifying the text generation function.
"""

#5.3.3 Modifying the text generation function

#let's combine these two to create a new generate function --- see utility.py
from utility import generate


torch.manual_seed(123)
token_ids = generate(
 model=model,
 idx=text_to_token_ids("Every effort moves you", tokenizer),
 max_new_tokens=15,
 context_size=GPT_CONFIG_124M["context_length"],
 top_k=25,
 temperature=1.4
)
print("Output text:\n", token_ids_to_text(token_ids, tokenizer)) # Every effort moves youlit terrace.
