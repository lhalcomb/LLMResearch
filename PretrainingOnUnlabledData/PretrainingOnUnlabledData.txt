
Thus far, we have implemented the data sampling and attention mechanism and
coded the LLM architecture. It is now time to implement a training function and
pretrain the LLM. We will learn about basic model evaluation techniques to measure the quality of the generated text, which is a requirement for optimizing the
LLM during the training process. Moreover, we will discuss how to load pretrained
weights, giving our LLM a solid starting point for fine-tuning. Figure 5.1 lays out
our overall plan, highlighting what we will discuss in this chapter.

We will cover the following:

    -- Computing the training and validation set losses
    to assess the quality of LLM-generated text during training

    --  Implementing a training function and pretraining
    the LLM

    --  Saving and loading model weights to continue
    training an LLM

    -- Loading pretrained weights from OpenAI

Summary:

    -- When LLMs generate text, they output one token at a time.

    -- By default, the next token is generated by converting the model outputs into
    probability scores and selecting the token from the vocabulary that corresponds
    to the highest probability score, which is known as “greedy decoding.”

    -- Using probabilistic sampling and temperature scaling, we can influence the
    diversity and coherence of the generated text.

    -- Training and validation set losses can be used to gauge the quality of text generated by LLM during training.

    -- Pretraining an LLM involves changing its weights to minimize the training loss.

    -- The training loop for LLMs itself is a standard procedure in deep learning,
    using a conventional cross entropy loss and AdamW optimizer.

    -- Pretraining an LLM on a large text corpus is time- and resource-intensive, so we
    can load openly available weights as an alternative to pretraining the model on
    a large dataset ourselves.